<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="AR Meal Assistance.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AR Meal Assistance</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://soobinpark.info">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AR Meal Assistance</h1>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <span style="color:lightblue; font-weight:bold"> -->

<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Overview</h2>
        <div class="content has-text-justified">
          <p>
            We have investigated how to convey food information in virtual reality settings focusing on the design of interaction to simulate and be aware of how to provide information and instructions during mealtime experiences of people with visual impairments (see <a href="https://www.soobinpark.info/Meal_Assistance/">VR Meal Assistance</a> for more details).
            However, to be used in a real-world dining environment, I started to develop a system for smartphone-based applications and created an <span style="color:blue; font-weight:bold">AR-based hand-to-food guidance system</span> for mobile platforms.
            The system basically follows the process we have done in the VR environment, but there is a difference in that it is based on augmented reality.
            The system supports two types of assistance: (1) providing dish-related information on a table, and (2) guiding users’ hand towards a particular dish.
            <BR>
            <BR>
            The strength of the system primarily lies in its user-friendly input and manipulation methods, which ultimately eliminates the need for hand to manipulate devices, enabling users to enjoy their food without distraction.
          </p>
        </div>
      </div>
    </div>
  </div>

    <!--/ Abstract. -->

  <br>
  <br>

  <div class="container">
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-2">Motivation</h2>
      <div class="content has-text-justified">
        <p>
          As this AR version project is an extension of the VR version of  Meal Assistance system, see <a href="https://www.soobinpark.info/Meal_Assistance/">Qualitative research</a> for background and motivation of the project.
        </p>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column is-8">
        <h2 class="title is-2">AR Implementation</h2>

        <div class="columns is-centered">
          <div class="content has-text-justified">
            <!-- <p style="color:lightgray;">
              The system basically follows the process we have done in the <a href="https://www.soobinpark.info/Meal_Assistance/">VR environment</a>, but there is a difference in that it is based on augmented reality and real-world settings.
            </p> -->
            <p>
              A mealtime assistance system for people with visual impairments involved AR-based hand-to-food guidance through auditory cues. With real-time 3D hand tracking and point-cloud tracking of the environment, my mobile application was designed to assist visually impaired persons’ ability to navigate 3D spaces and locate food---thus helping resolve an important last-few-meters wayfinding problem.
              Real-time speech recognition enabled natural communication between the app and its users, and allowed users to identify and retrieve the food they wanted.
            <p>
              The system was implemented in the environment of Unity 3D engine and built onto our iOS devices through Xcode.
          </div>
        </div>
        <BR>

          <h3 class="title is-4">Listening to explanation about the location.</h3>
          <div class="columns is-centered">
          <div class="content has-text-justified">
            <p>
              First is the method of assisting in distinguishing the overall location of the dishes when it is served.
              In order to identify the location of the objects in space, I used a smartphone that has a depth camera-equipped (e.g., iPhone X) and perform 3D object detection to check the positions of the dish in 3-dimensional space.
              When the dishes' location is identified, I provide information in the most preferred way by the PVI, which is providing the dishes' directions in relation to the clock vocally.
              Also, if the PVI wishes to know the location of a particular dish during a meal, I use speech as the input modality and provide dishes' direction through vocal assistance.
            </p>
              <p>
                <b><u>3D object tracking</u></b>
                <BR> In order to track objects in 3D space and in real-time, I used Apple’s augmented reality framework, <a href="https://developer.apple.com/documentation/arkit/content_anchors/scanning_and_detecting_3d_objects">ARKit</a>, to enable object detection.
                  I scanned Real-World Objects with an iOS App and preloaded the feature-point-scans into the application.
              </p>

                <div class="content">
                  <!-- <img src="./static/images/setting.png" alt="intro"> -->
                  <video id="teaser" autoplay muted loop height="100%">
                    <source src="./static/videos/scan.mp4"
                            type="video/mp4">
                  </video>
                  <p style="text-align:left;">
                    As the game starts, the intro screen comes out. In Intro, a typhoon-related news appears on the TV screen at home.
                  </p>
                </div>

              <p>
                <b><u>Speech modality</u></b>
                <BR> To get the voice command from the users and provide the output to the users, I used <a href="https://www.ibm.com/watson">IBM’s Watson</a> to receive users’ voice input through the mobile phone and convert it to text.
                I also used a text-to-speech module to provide requested information verbally to the users.
              </p>
            </div>
          </div>

            <!-- <h3 class="title is-4" align="left">Leading the hands towards the dishes.</h3> -->
            <BR>
            <h3 class="title is-4">Leading the hands towards the dishes.</h3>
            <div class="columns is-centered">
            <div class="content has-text-justified">
              <p>
                Instead of directing the PVI hand to locate the dishes from others, we can enable them to figure out the accurate dishes' location without any help from others by having themselves touch the dishes.
                At this point, incorporating augmented reality technology, virtual objects are displayed on each dish that is detected.
                Also, by performing real-time hand tracking, hand can also be detected in 3D space.
                With interactions between virtual objects of hands and dishes, the overall location of the dishes can be distinguished.
                If the PVI wish to approach a specific dish during a meal, we can provide audio feedback based on the spatial distance between the hand and the dishes so that the user can do way-finding and guide the hand to the location of the specific dish.
                <!-- With the method presented above, PVI can locate dishes without any help from others and satisfy their desire for independent eating., and we look forward to eliminating the trouble of PVI having to try all the dishes when the help from others are unavailable. -->
              </p>
                <p>
                  <b><u>3D hand tracking</u></b>
                  <BR> For the 3D hand tracking and interaction, I used <a href="https://www.manomotion.com/">Manomotion</a> to be used in AR frameworks on mobile platforms.
                    I was able to retrieve the 3D position of users' hand with high precision.
                </p>
                <p>
                  <b><u>Calculating distance between Hand and Dishes and providing Feedback</u></b>
                  <BR> I applied an audio feedback emitting beeping sound inversely proportional to distance to guide users’ hand to food, and haptic feedback to confirm that users have reached the desired dishes.
                    When their hand approaches dishes, it is similar to parking sensors which alerts users with signals.
                    As a result, user's hand can approach the object of interest.
                </p>
              </div>
            </div>

          <div class="column">
            <div class="content">
                <img src="./static/images/setting.png" alt="setting">
            </div>
          </div>

        <!--/ Matting. -->
        <div class="columns is-centered">
        <div class="content has-text-justified">
          <p>
            <b>1. Dish-related Information.</b> The system provides the <b>overview</b> information of the dishes on a table when it detects the users’ voice command saying "overview". It informs the total number of dishes followed by the dish arrangement (e.g., "There are in total 6 dishes straightly lined up"), and how dishes are spread about (e.g., "Starting from 10 o’clock to 2 o’clock.").
            Lastly, it notifies the name of each dish one at a time from left to right.
          </p>
          <p>
            <b>2. Hand Guidance.</b> The system also helps the users to find a particular dish on a table with audio feedback. For instance, when a user asks for the location (e.g., "Where is the sandwich?"), it first verbally conveys the relative direction of the dish (e.g. "It’s at 2 o’clock"), which we refer as <b>verbal guidance</b>.
            Then it plays beeps where its frequency gets higher as a user’s hand gets closer to the target (<b>beeping guidance</b>).
            If the user finds a dish other than the target, the system informs the location of the target dish in a relative direction from the user, such as whether it is at the left or the right of the user. In addition, the system cautions the user if the temperature of the target dish is too hot.
          </p>
        </div>
      </div>

      </div>
    </div>
  </div>
</div>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link" href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:center;">
            The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
